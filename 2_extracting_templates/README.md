# Extracting template and module pages from the Wikipedia's full history dump

After downloading the XML dump, the next step is to extract the template and module pages from the dump files in order to be inserted in MySQL database and used by the Mediawiki software when parsing the pages from Wikitext to HTML. Because the templates and modules are scattered across the dump files, every file needs to be uncompressed, the template and module pages from the files must be extracted and then the uncompressed files deleted. To speed up this process, 10 files are processed in parallel. On the start 10 files are uncompressed and the extraction process is started, whenever an extraction process is completed, the script detects it, uncompresses a new file and starts the process for the new file. Because the files are huge and don't fit in main memory, a SAX parser is used which processes XML in a streaming mode. After the extraction process is done, there are 558 different MySQL insert files generated (one for every XML file), the `generate_one_sql.py` scripts takes all the MySQL insertion files, concatenates them and create one big MySQL insert file containing all the templates and modules found in the XML dump which is later used to create the MySQL template and module database.
