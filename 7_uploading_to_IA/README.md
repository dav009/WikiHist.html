# Uploading the data to Internet Archive

After the processing is done, the failed pages are also processed, and all the results were stored in one place - the NFS, the data was uploaded to Internet Archive with these scripts. First, the `upload_every_file.sh` creates items on Internet Archive for every result directory (for every XML file from the dump), and uploads the json files containing the pages, if there is an error during the uploading of the files, the name of the file is saved to a log. Afterwards, when the `upload_every_file.sh` script is done, the `upload_errors.sh` is run to upload the failed files which are saved in the log.
